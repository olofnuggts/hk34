{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbe4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# This converts the images (0-255) into Tensors (0.0 - 1.0)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download and load training data\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# Create a loader to feed data in batches of 32 images at a time\n",
    "train_loader: DataLoader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader: DataLoader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b2e4b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAADyFJREFUeJzt3H2s1/P/x/HnR/lWNFKdtmYrO8qI2hCZpXIxuYidRLGZFeuPmJktlyMMw+b6uomVZTsLJWLSRq7WSq62Q5GLZq5Pkmsafb5/fH+e41c4r0/nopPbbfPP8X70fjmqe+9TvSvVarUaABARO3T0AQDYdogCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkC26W1a9dGpVKJG2+8sdW+zaVLl0alUomlS5e22rcJ2xpRYJsxe/bsqFQqsXLlyo4+SptpbGyMAw44ILp37x51dXVx1llnxbp16zr6WJBEAdrJPffcE6eddlr07t07br755pg6dWo0NjbGkUceGT///HNHHw8iIqJrRx8A/g02btwYl156aYwaNSqWLFkSlUolIiIOPfTQOOGEE+K+++6Lc889t4NPCZ4U6GQ2btwYM2bMiAMPPDB23XXX2HnnneOwww6L55577i83t9xySwwcODB69OgRo0ePjqamps2uWb16dZx88snRu3fv6N69ewwfPjwef/zxfzzPjz/+GKtXr/7HLwE1NTXFhg0bYtKkSRmEiIhx48ZFz549o7Gx8R/vBe1BFOhUvv3225g1a1aMGTMmbrjhhrjyyiujubk5xo4dG2+88cZm1z/44INx++23xznnnBOXXHJJNDU1xRFHHBFffPFFXvPWW2/FIYccEqtWrYqLL744brrppth5552joaEhFixY8LfnWbFiReyzzz5x5513/u11v/zyS0RE9OjRY7N/16NHj3j99ddj06ZNLfgMQNvy5SM6ld122y3Wrl0b//nPf/JjU6dOjb333jvuuOOOuP/++/90/XvvvRdr1qyJ3XffPSIijjnmmBgxYkTccMMNcfPNN0dExHnnnRcDBgyIV155Jbp16xYREWeffXaMHDkyLrroohg/fvxWn3vw4MFRqVTi5ZdfjilTpuTH33nnnWhubo6IiK+//jr69Omz1feCreFJgU6lS5cuGYRNmzbF+vXr49dff43hw4fHa6+9ttn1DQ0NGYSIiIMPPjhGjBgRTz31VERErF+/Pp599tmYOHFifPfdd7Fu3bpYt25dfPXVVzF27NhYs2ZNfPLJJ395njFjxkS1Wo0rr7zyb8/dt2/fmDhxYsyZMyduuumm+OCDD+LFF1+MSZMmxY477hgRET/99FPppwNanSjQ6cyZMyeGDRsW3bt3jz59+kRdXV08+eST8c0332x27eDBgzf72F577RVr166NiP89SVSr1bj88sujrq7uT/9cccUVERHx5Zdftsq5Z86cGccdd1xMnz499txzzxg1alQMHTo0TjjhhIiI6NmzZ6vcB7aGLx/RqcydOzcmT54cDQ0NccEFF0S/fv2iS5cucd1118X7779f/O39/nX86dOnx9ixY7d4zaBBg7bqzL/bddddY+HChfHRRx/F2rVrY+DAgTFw4MA49NBDo66uLnr16tUq94GtIQp0Ko888kjU19fH/Pnz//SneH7/Vf3/t2bNms0+9u6778Yee+wRERH19fUREbHjjjvGUUcd1foH3oIBAwbEgAEDIiJiw4YN8eqrr8aECRPa5d7wT3z5iE6lS5cuERFRrVbzY8uXL49ly5Zt8frHHnvsT78nsGLFili+fHkce+yxERHRr1+/GDNmTMycOTM+++yzzfa//ybwX2npH0n9K5dcckn8+uuvcf7559e0h9bmSYFtzgMPPBBPP/30Zh8/77zzYty4cTF//vwYP358HH/88fHhhx/GvffeG0OGDInvv/9+s82gQYNi5MiRMW3atPjll1/i1ltvjT59+sSFF16Y19x1110xcuTIGDp0aEydOjXq6+vjiy++iGXLlsXHH38cb7755l+edcWKFXH44YfHFVdc8Y+/2Xz99ddHU1NTjBgxIrp27RqPPfZYPPPMM3HNNdfEQQcd1PJPELQhUWCbc88992zx45MnT47JkyfH559/HjNnzozFixfHkCFDYu7cufHwww9v8UV1Z5xxRuywww5x6623xpdffhkHH3xw3HnnndG/f/+8ZsiQIbFy5cq46qqrYvbs2fHVV19Fv379Yv/9948ZM2a02n/X0KFDY8GCBfH444/Hb7/9FsOGDYt58+bFKaec0mr3gK1Vqf7xORyAfzW/pwBAEgUAkigAkEQBgCQKACRRACC1+O8p/PGVAgB0Pi35GwieFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUteOPgD8kxEjRhRvTj/99OLN6NGjizf77rtv8aZW06dPL958+umnxZuRI0cWb+bOnVu8Wb58efGGtudJAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASJVqtVpt0YWVSlufhe3cpEmTatrddtttxZu+ffsWb2r5Pr506dLiTV1dXfEmImLIkCE17UrV8nl4+OGHizennnpq8Yat05Kf7j0pAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgde3oA9DxunYt/24wfPjw4s19991XvImI2GmnnYo3L7zwQvHm6quvLt689NJLxZtu3boVbyIi5s2bV7w5+uija7pXqZUrV7bLfWh7nhQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJC8EI84/fTTizezZs1qg5Ns2ZIlS4o3kyZNKt58++23xZta1HK2iPZ7ud3HH39cvJkzZ04bnISO4EkBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgCpUq1Wqy26sFJp67PQCq6++urizaWXXlq8aeF3mz+5++67izcREZdddlnxpr1ebleLVatW1bQbPHhwK59kyyZMmFC8WbhwYRuchNbWkh+3nhQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDUtaMPwJbNmDGjpl0tbzzduHFj8Wbx4sXFm4suuqh4ExHx008/1bQr1b179+LN0UcfXbwZMGBA8SaitjcVX3PNNcUbbzz9d/OkAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAVKlWq9UWXVjDy7j4n169ehVvVq9eXdO9+vbtW7xZtGhR8aahoaF4054GDRpUvHnooYeKNwceeGDxplaPPvpo8ebMM88s3vzwww/FGzqHlvx070kBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDJC/HaQb9+/Yo3n376aRucZMvq6+uLNz///HPxZsqUKcWbiIgTTzyxeLPffvsVb3r27Fm8aeEPn63eREScdNJJxZsnnniipnuxffJCPACKiAIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQPJCvHbQq1ev4s2qVatqulddXV3xppb/t7W+1K291PJCwVo+D/379y/eNDc3F29qvRf8kRfiAVBEFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUteOPsC/wYYNG4o3DQ0NNd1r0aJFxZvevXsXb95///3izcKFC4s3ERGzZ88u3qxfv75409jYWLyp5SV1tdwH2osnBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIHlL6jZq+fLlNe3q6upa+SSd06hRo4o3o0ePLt5s2rSpePPBBx8Ub6C9eFIAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEDyQjy2Sz169Cje1PJyu2q1WrxpbGws3kB78aQAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBUqbbwjV6VSqWtzwId6rfffive1PJCvP79+xdvIiKam5tr2sHvWvL91ZMCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBS144+ALSFsWPHdvQRoFPypABAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOSFeGyX6uvrO/oI0Cl5UgAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJK3pLJdevHFF4s3O+xQ/mukTZs2FW9gW+ZJAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAyQvx2C41NTUVb9asWVO8qa+vL97sueeexZuIiObm5pp2UMKTAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUqVarVZbdGGl0tZngQ41efLk4s2sWbOKN88//3zxJiLi3HPPLd68/fbbNd2L7VNLfrr3pABAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOSFePB/dtlll+LNvHnzijdHHXVU8SYiYv78+cWbKVOmFG9++OGH4g2dgxfiAVBEFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkLwlFbZCLW9Wvfbaa2u617Rp04o3w4YNK968/fbbxRs6B29JBaCIKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJC/EA/iX8EI8AIqIAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBA6trSC1v43jwAOjFPCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCk/wI4JPMQokgraAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Grab the raw image tensor\n",
    "image = train_data.data[4]\n",
    "label = train_data.targets[4]\n",
    "\n",
    "# 2. Plot it\n",
    "plt.imshow(image, cmap=\"gray\") # cmap=\"gray\" forces black & white\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis(\"off\") # Optional: Hides the ruler numbers\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e61875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(in_features=64*7*7, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "neuralNetwork = NeuralNetwork().to(device)\n",
    "neuralNetwork\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca8f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CrossEntropyLoss is standard for classification (choosing 1 out of N categories)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(neuralNetwork.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8edbf7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 2.30520, Acc: 8.80% | Test Loss: 2.30360, Test Acc: 14.00%\n",
      "Epoch: 100 | Loss: 2.25080, Acc: 35.30% | Test Loss: 2.24424, Test Acc: 41.00%\n",
      "Epoch: 200 | Loss: 2.07757, Acc: 63.70% | Test Loss: 2.06719, Test Acc: 65.00%\n",
      "Epoch: 300 | Loss: 1.24070, Acc: 76.10% | Test Loss: 1.23687, Test Acc: 75.00%\n",
      "Epoch: 400 | Loss: 0.59877, Acc: 85.00% | Test Loss: 0.65402, Test Acc: 79.00%\n",
      "Epoch: 500 | Loss: 0.43112, Acc: 87.90% | Test Loss: 0.49091, Test Acc: 85.00%\n",
      "Epoch: 600 | Loss: 0.35345, Acc: 89.60% | Test Loss: 0.40834, Test Acc: 85.00%\n",
      "Epoch: 700 | Loss: 0.30433, Acc: 91.90% | Test Loss: 0.35344, Test Acc: 88.00%\n",
      "Epoch: 800 | Loss: 0.26785, Acc: 92.90% | Test Loss: 0.31640, Test Acc: 89.00%\n",
      "Epoch: 900 | Loss: 0.23838, Acc: 93.60% | Test Loss: 0.29072, Test Acc: 88.00%\n",
      "Epoch: 1000 | Loss: 0.21309, Acc: 94.30% | Test Loss: 0.27089, Test Acc: 88.00%\n",
      "Epoch: 1100 | Loss: 0.19091, Acc: 94.50% | Test Loss: 0.25494, Test Acc: 89.00%\n",
      "Epoch: 1200 | Loss: 0.17091, Acc: 94.90% | Test Loss: 0.24378, Test Acc: 90.00%\n",
      "Epoch: 1300 | Loss: 0.15270, Acc: 95.70% | Test Loss: 0.23466, Test Acc: 89.00%\n",
      "Epoch: 1400 | Loss: 0.13588, Acc: 96.00% | Test Loss: 0.22751, Test Acc: 89.00%\n",
      "Epoch: 1500 | Loss: 0.12045, Acc: 97.00% | Test Loss: 0.22195, Test Acc: 90.00%\n",
      "Epoch: 1600 | Loss: 0.10643, Acc: 97.30% | Test Loss: 0.21697, Test Acc: 89.00%\n",
      "Epoch: 1700 | Loss: 0.09370, Acc: 97.60% | Test Loss: 0.21342, Test Acc: 89.00%\n",
      "Epoch: 1800 | Loss: 0.08215, Acc: 98.00% | Test Loss: 0.21140, Test Acc: 88.00%\n",
      "Epoch: 1900 | Loss: 0.07185, Acc: 98.50% | Test Loss: 0.20927, Test Acc: 89.00%\n",
      "Epoch: 2000 | Loss: 0.06279, Acc: 98.60% | Test Loss: 0.20688, Test Acc: 89.00%\n",
      "Epoch: 2100 | Loss: 0.05494, Acc: 98.90% | Test Loss: 0.20540, Test Acc: 90.00%\n",
      "Epoch: 2200 | Loss: 0.04817, Acc: 99.50% | Test Loss: 0.20419, Test Acc: 90.00%\n",
      "Epoch: 2300 | Loss: 0.04234, Acc: 99.70% | Test Loss: 0.20293, Test Acc: 90.00%\n",
      "Epoch: 2400 | Loss: 0.03733, Acc: 99.80% | Test Loss: 0.20203, Test Acc: 90.00%\n",
      "Epoch: 2500 | Loss: 0.03306, Acc: 99.90% | Test Loss: 0.20146, Test Acc: 90.00%\n",
      "Epoch: 2600 | Loss: 0.02939, Acc: 99.90% | Test Loss: 0.20060, Test Acc: 90.00%\n",
      "Epoch: 2700 | Loss: 0.02626, Acc: 100.00% | Test Loss: 0.20023, Test Acc: 90.00%\n",
      "Epoch: 2800 | Loss: 0.02356, Acc: 100.00% | Test Loss: 0.20009, Test Acc: 91.00%\n",
      "Epoch: 2900 | Loss: 0.02125, Acc: 100.00% | Test Loss: 0.20002, Test Acc: 91.00%\n",
      "Epoch: 3000 | Loss: 0.01925, Acc: 100.00% | Test Loss: 0.19998, Test Acc: 91.00%\n",
      "Epoch: 3100 | Loss: 0.01752, Acc: 100.00% | Test Loss: 0.19997, Test Acc: 91.00%\n",
      "Epoch: 3200 | Loss: 0.01601, Acc: 100.00% | Test Loss: 0.20017, Test Acc: 91.00%\n",
      "Epoch: 3300 | Loss: 0.01469, Acc: 100.00% | Test Loss: 0.20049, Test Acc: 91.00%\n",
      "Epoch: 3400 | Loss: 0.01353, Acc: 100.00% | Test Loss: 0.20067, Test Acc: 91.00%\n",
      "Epoch: 3500 | Loss: 0.01250, Acc: 100.00% | Test Loss: 0.20092, Test Acc: 91.00%\n",
      "Epoch: 3600 | Loss: 0.01159, Acc: 100.00% | Test Loss: 0.20104, Test Acc: 91.00%\n",
      "Epoch: 3700 | Loss: 0.01078, Acc: 100.00% | Test Loss: 0.20122, Test Acc: 91.00%\n",
      "Epoch: 3800 | Loss: 0.01006, Acc: 100.00% | Test Loss: 0.20168, Test Acc: 91.00%\n",
      "Epoch: 3900 | Loss: 0.00940, Acc: 100.00% | Test Loss: 0.20205, Test Acc: 91.00%\n",
      "Epoch: 4000 | Loss: 0.00882, Acc: 100.00% | Test Loss: 0.20236, Test Acc: 91.00%\n",
      "Epoch: 4100 | Loss: 0.00829, Acc: 100.00% | Test Loss: 0.20256, Test Acc: 92.00%\n",
      "Epoch: 4200 | Loss: 0.00781, Acc: 100.00% | Test Loss: 0.20300, Test Acc: 92.00%\n",
      "Epoch: 4300 | Loss: 0.00737, Acc: 100.00% | Test Loss: 0.20337, Test Acc: 92.00%\n",
      "Epoch: 4400 | Loss: 0.00697, Acc: 100.00% | Test Loss: 0.20373, Test Acc: 92.00%\n",
      "Epoch: 4500 | Loss: 0.00661, Acc: 100.00% | Test Loss: 0.20426, Test Acc: 92.00%\n",
      "Epoch: 4600 | Loss: 0.00627, Acc: 100.00% | Test Loss: 0.20477, Test Acc: 92.00%\n",
      "Epoch: 4700 | Loss: 0.00597, Acc: 100.00% | Test Loss: 0.20522, Test Acc: 92.00%\n",
      "Epoch: 4800 | Loss: 0.00568, Acc: 100.00% | Test Loss: 0.20567, Test Acc: 92.00%\n",
      "Epoch: 4900 | Loss: 0.00542, Acc: 100.00% | Test Loss: 0.20601, Test Acc: 92.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Add channel dimension (1) expected by Conv2d: (batch, 1, 28, 28)\n",
    "X_train = (train_data.data[:1000].unsqueeze(1).float() / 255.0).to(device)\n",
    "X_test = (test_data.data[:100].unsqueeze(1).float() / 255.0).to(device)\n",
    "y_train = train_data.targets[:1000].long().to(device)\n",
    "y_test = test_data.targets[:100].long().to(device)\n",
    "\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    neuralNetwork.train()\n",
    "\n",
    "    # Forward pass\n",
    "    y_logits = neuralNetwork(X_train)\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) \n",
    "\n",
    "    # Calculate loss/accuracy\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_score(y_true=y_train.cpu(), y_pred=y_pred.cpu())\n",
    "\n",
    "    # Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    neuralNetwork.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Forward pass\n",
    "        test_logits = neuralNetwork(X_test)\n",
    "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "        # Calculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_score(y_true=y_test.cpu(), y_pred=test_pred.cpu())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2%} | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517ec23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "DEBUG: Data Range is 0.00 to 1.00\n",
      "Train Loss: 0.32770 | Train Acc: 90.26%\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Loss: 0.30137 | Train Acc: 91.00%\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Loss: 0.27961 | Train Acc: 91.67%\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Loss: 0.25968 | Train Acc: 92.26%\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Loss: 0.24163 | Train Acc: 92.79%\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Loss: 0.22463 | Train Acc: 93.32%\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Loss: 0.20961 | Train Acc: 93.77%\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Loss: 0.19558 | Train Acc: 94.22%\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Loss: 0.18283 | Train Acc: 94.53%\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Loss: 0.17127 | Train Acc: 94.96%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    neuralNetwork.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    # X is the images y is the labels\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        # --- SANITY CHECK (Verifies data is correct) ---\n",
    "        if epoch == 0 and batch == 0:\n",
    "            print(f\"DEBUG: Data Range is {X.min():.2f} to {X.max():.2f}\")\n",
    "            if X.max() > 1.0:\n",
    "                print(\"!! WARNING: Data is not normalized (0-255). Check transforms !!\")\n",
    "        # -----------------------------------------------\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward\n",
    "        y_pred = neuralNetwork(X)\n",
    "        \n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Accuracy\n",
    "        train_acc += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate averages\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader.dataset) #type: ignore\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.5f} | Train Acc: {train_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4210e74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad(): # Tell PyTorch we don't need gradients for this (saves memory)\n",
    "    images = images.to(device)  # Move images to the same device as the model\n",
    "    outputs = neuralNetwork(images)\n",
    "    # The prediction is the index with the highest score\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Show the first 5 images and their predictions\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(1, 5, i+1, xticks=[], yticks=[])\n",
    "    # Convert tensor image back to numpy for display\n",
    "    # .cpu() moves it back to system memory\n",
    "    img = images[i].cpu().squeeze().numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f\"Pred: {predicted[i].item()}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
